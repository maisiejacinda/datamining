# -*- coding: utf-8 -*-
"""berita2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HnOnDwaNt1wkg__JO7i5UqB-YZ_j5YEi
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("hoax_dataset_with_text.csv")

df.head()

print(df.columns)

def extract_label(judul):
    if judul.startswith("[SALAH]"):
        return "hoax"
    else:
        return "valid"

df['label'] = df['judul'].apply(extract_label)

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(data=df, x='label')
plt.title("Distribusi Label Hoaks vs Valid")
plt.show()

df.info()

print(df.columns)

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')

df['teks'] = df['teks'].astype(str)

tokenized = tokenizer(
    list(df['teks']),
    padding='max_length',
    truncation=True,
    max_length=128,
    return_tensors='pt'
)

df['teks'] = df['teks'].astype(str)

tokenized = tokenizer(
    list(df['teks']),
    padding='max_length',
    truncation=True,
    max_length=128,
    return_tensors='pt'
)

from sklearn.model_selection import train_test_split
import torch

df['label'] = df['label'].map({'hoax': 1, 'valid': 0})

input_ids = tokenized['input_ids']
attention_mask = tokenized['attention_mask']
labels = torch.tensor(df['label'].values)

X_train_ids, X_test_ids, X_train_mask, X_test_mask, y_train, y_test = train_test_split(
    input_ids,
    attention_mask,
    labels,
    test_size=0.2,
    stratify=labels,
    random_state=42
)

df = df.dropna().drop_duplicates()

# Menghapus data kosong dan duplikat
df = df.dropna().drop_duplicates()

# Menyimpan hasil preprocessing ke file CSV
df.to_csv('preprocessed_data.csv', index=False)
print("Preprocessing selesai, file disimpan sebagai preprocessed_data.csv")
